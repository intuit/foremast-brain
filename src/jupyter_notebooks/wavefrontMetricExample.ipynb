{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wavefront_api_client as wave_api\n",
    "import time\n",
    "import datetime as dt\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from mlalgms.statsmodel import detectAnomalies\n",
    "from utils.converterutils import addHeader\n",
    "from dateutil.parser import parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE\n",
    "base_url = 'WAVEFRONT_ENDPOINT'\n",
    "\n",
    "api_key = 'WAVEFRONT_ACCESS_TOKEN'\n",
    "\n",
    "\n",
    "#will return % utilization of cpu all sources matching tags\n",
    "#cpu_query = '100 - rate(ts(\"collectd.aggregation-cpu-average.cpu-idle\", bu=sbg and env=prd and app=qbo and pool=app and location=qcy and source=pprdqboas83b.ie.intuit.net or source=pprdqboas83y.ie.intuit.net))'\n",
    "\n",
    "#memory metric to scrape\n",
    "#will return memory utilization for all sources matching tags\n",
    "#memory_query = 'ts(\"collectd.memory.memory-used\", bu=sbg and app=qbo and env=prd and pool=app and location=qcy and source=pprdqboas83b.ie.intuit.net or source=pprdqboas83y.ie.intuit.net)'\n",
    "\n",
    "app=\"YOUR_APP_NAME\"\n",
    "err_5xx_query = 'sum(ts(telegraf.http.server.requests.count, env=\"prd\" and app=\"[APP]\" and status=\"5*\"))'\n",
    "err_4xx_query = 'sum(ts(telegraf.http.server.requests.count, env=\"prd\" and app=\"[APP]\" and status=\"4*\"))'\n",
    "avg_latency_query = 'avg(ts(telegraf.http.server.requests.mean, env=\"prd\" and status=\"2*\" and app=\"[APP]\"), app)'\n",
    "\n",
    "avg_error_query ='avg(align(60s, mean, ts(\"appdynamics.apm.errors.errors_per_min\", env=\"prd\" and app=\"[APP]\")), app)'\n",
    "\n",
    "\n",
    "#change to give the desired destination for resulting csv file\n",
    "err_5xx = './err_5xx_data.csv'\n",
    "err_4xx = './err_4xx_data.csv'\n",
    "avg_latency = './avg_latency_data.csv'\n",
    "\n",
    "avg_error = './avg_error_data.csv'\n",
    "\n",
    "\n",
    "#time in hours to capture in query window\n",
    "#  easiest to multiply it out for longer periods\n",
    "#    eg. 1 week = 7 (days/week) * 24 (hours/day)\n",
    "query_window = 24*7*2\n",
    "\n",
    "#calculate start and end times in ms\n",
    "end_time = time.time() * 1000\n",
    "#end_time = (24*8 * 60 * 60) * 1000\n",
    "start_time = (end_time) - (query_window * 60 * 60) * 1000\n",
    "end_time = start_time +(24*5 * 60 * 60) * 1000\n",
    "\n",
    "#desired query granularity (s, m, h, or d)\n",
    "query_granularity = 'm'\n",
    "\n",
    "def retrieveQueryUrl(app, url):\n",
    "    return url.replace('[APP]', app)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#issue the query through the wavefront query api\n",
    "def executeQuery( query, client, start_time, end_time):\n",
    "    query_api = wave_api.QueryApi(client)\n",
    "    app_query = retrieveQueryUrl(app, query)\n",
    "    result = query_api.query_api(app_query, str(start_time), query_granularity, e=str(end_time))\n",
    "    print(result)\n",
    "    return formatData(result)\n",
    "\n",
    "\n",
    "\n",
    "#enter resulting data into the dataframe with the timestamp as the index\n",
    "def formatData(result):\n",
    "    if result.timeseries is not None:\n",
    "        for entry in result.timeseries:\n",
    "            #server_name = entry.host\n",
    "            #label = entry.label            \n",
    "            data = np.array(entry.data)\n",
    "            \n",
    "            #location = entry.tags[u'location']\n",
    "            #app = entry.tags[u'app']\n",
    "            #bu = entry.tags[u'bu']\n",
    "            \n",
    "            idx = pd.Series(data[:,0])\n",
    "            \n",
    "            dtime = [dt.datetime.fromtimestamp(int(x)).strftime('%Y-%m-%d %H:%M:%S') for x in idx ]\n",
    "            dtime1 = [parse(d) for d in dtime]\n",
    "            y = data[:,1]\n",
    "            #df = addHeader (idx, y)\n",
    "            df = addHeader(dtime1,y)\n",
    "            #print(df.index)\n",
    "            return df\n",
    "            #df[app] =  pd.Series(data[:,1], index=data[:,0])\n",
    "            #return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing libraries\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import date2num\n",
    "from IPython.core.pylabtools import figsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showGraph(name,df):\n",
    "    ll = df.index.tolist()\n",
    "    plt_ds = [date2num(dd)  for dd in ll]\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.plot_date(plt_ds, df.y)\n",
    "    plt.title(name+' timeseries')\n",
    "    plt.xlabel('Time (minutes)')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#set up python Wavefront client\n",
    "config = wave_api.Configuration()\n",
    "config.host = base_url\n",
    "client = wave_api.ApiClient(configuration=config, header_name='Authorization', header_value='Bearer ' + api_key)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "err_5xx_df = executeQuery( err_5xx_query, client,start_time, end_time)\n",
    "showGraph(app+' err_5xx', err_5xx_df)\n",
    "err_5xx_df.to_csv(err_5xx)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_4xx_df = executeQuery(err_4xx_query, client,start_time, end_time)\n",
    "showGraph(app+' err_4xx', err_4xx_df)\n",
    "err_4xx_df.to_csv(err_4xx)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_latency_df = executeQuery( avg_latency_query, client,start_time, end_time)\n",
    "showGraph(app+' avg latency', avg_latency_df)\n",
    "avg_latency_df.to_csv(avg_latency)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_error_df = executeQuery( avg_error_query, client,start_time, end_time)\n",
    "showGraph('tds avg error', avg_error_df)\n",
    "avg_error_df.to_csv(avg_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlalgms.fbprophet  import predictNoneSeasonalityProphetLast\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totimestamp(np_dt):\n",
    "   return (np_dt - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlalgms.evaluator import ts_train_test_split\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = ts_train_test_split(avg_error_df, split_ratio=0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n",
      "<class 'datetime.datetime'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idx = train.index.get_values()\n",
    "p_ts = [totimestamp(d) for d in idx]\n",
    "print(type(p_ts[0]))\n",
    "p_utc = [datetime.utcfromtimestamp(int(d)) for d in p_ts]\n",
    "print(type(p_utc[0]))\n",
    "\n",
    "df_prophet = addHeader (p_ts, train.y.values, p_utc,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fbprophet import Prophet\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "PROPHET_PERIOD = 'period'\n",
    "PROPHET_FREQ = 'freq'\n",
    "DEFAULT_PROPHET_PERIOD =1\n",
    "DEFAULT_PROPHET_FREQ  ='T'\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictProphet(timeseries, period=1 ,frequence ='T', seasonality_name='', prior_scale=0.1, columnPosition=0):\n",
    "    prophet = Prophet()\n",
    "    if seasonality_name=='daily':\n",
    "        prophet.add_seasonality('daily', 1, fourier_order=1, prior_scale=prior_scale)\n",
    "    elif seasonality_name=='weekly':\n",
    "        prophet.add_seasonality('weekly', 7, fourier_order=3, prior_scale=prior_scale)\n",
    "    elif seasonality_name=='monthly':\n",
    "        prophet = Prophet(weekly_seasonality=False)\n",
    "        prophet.add_seasonality('monthly', 30.5, fourier_order=5,prior_scale=prior_scale)\n",
    "    elif seasonality_name=='yearly':\n",
    "        prophet.add_seasonality('yearly', 365, fourier_order=10,prior_scale=prior_scale)    \n",
    "    prophet.fit(timeseries)\n",
    "    future = prophet.make_future_dataframe(periods=period,freq=frequence)\n",
    "    forecast = prophet.predict(future)\n",
    "    if columnPosition == 0 :\n",
    "        return forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
    "    else :\n",
    "        return forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']][columnPosition:]\n",
    "    \n",
    "def prophetPredictUpperLower(timeseries, period=1,frequence ='T', zscore = 2,seasonality_name='',nd_return=False, prior_scale=0.1, columnPosition=0):\n",
    "    df = timeseries.copy()\n",
    "    df.dropna()\n",
    "    orig_len = len(timeseries)\n",
    "    fc = predictProphet(df, period, frequence,seasonality_name, prior_scale,columnPosition)\n",
    "    after_len = len(fc)\n",
    "    print(orig_len ,'  ', after_len)\n",
    "    if seasonality_name=='' or nd_return :  \n",
    "        mean = fc[orig_len:].yhat_lower.mean()\n",
    "        std = fc[orig_len:].yhat_lower.std()\n",
    "        return mean-zscore*std, mean+zscore*std\n",
    "    return fc[['ds','yhat_lower','yhat_upper']][orig_len:]\n",
    "    #return forecast.yhat_lower[size-1], forecast.yhat_upper[size-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5602    7042\n"
     ]
    }
   ],
   "source": [
    "dd = prophetPredictUpperLower(df_prophet,1440, 'T',2,'daily') \n",
    "lower, high = prophetPredictUpperLower(df_prophet,1440, 'T',2,'daily',True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx = test.index.get_values()\n",
    "p_ts = [totimestamp(d) for d in idx]\n",
    "p_utc = [datetime.utcfromtimestamp(int(d)) for d in p_ts]\n",
    "\n",
    "\n",
    "df_prophet_test = addHeader (p_ts, test.y.values, p_utc,False)\n",
    "\n",
    "test_predict= pd.merge(dd, df_prophet_test, on='ds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_predict.plot(style = \".\", figsize = (14,8), title = \"Anomaly Detection Using Prophet: CPU Utilization Forecasting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_UPPER_BOUND =1\n",
    "def detectLowerUpperAnomalies(df , bound=IS_UPPER_BOUND, returnAnomaliesOnly= True):\n",
    "\n",
    "    ts=[]\n",
    "    adata=[]\n",
    "    anomalies=[]\n",
    "    myshape = df.shape\n",
    "    nrow = myshape[0]\n",
    "    for i in range(nrow):  \n",
    "         isAnomaly = False\n",
    "         if (not returnAnomaliesOnly):\n",
    "            ts.append(df['ds'][i])\n",
    "            adata.append(df['y'][i])\n",
    "         if bound==IS_UPPER_BOUND:\n",
    "            if df['y'][i] > df['yhat_upper'][i]:\n",
    "                if returnAnomaliesOnly:\n",
    "                    ts.append(df['ds'][i])\n",
    "                    adata.append(df['y'][i])\n",
    "                isAnomaly = True\n",
    "         elif bound==IS_LOWER_BOUND:\n",
    "            if df['y'][i] < df['yhat_lower'][i]:\n",
    "                if returnAnomaliesOnly:\n",
    "                    ts.append(df['ds'][i])\n",
    "                    adata.append(df['y'][i])\n",
    "                isAnomaly = True            \n",
    "         else:   \n",
    "            if df['y'][i] > df['yhat_upper'][i] or df['y'][i] < df['yhat_lower'][i]:\n",
    "                if returnAnomaliesOnly:\n",
    "                    ts.append(df['ds'][i])\n",
    "                    adata.append(df['y'][i])\n",
    "                isAnomaly = True\n",
    "                    \n",
    "         if returnAnomaliesOnly:\n",
    "            if isAnomaly:\n",
    "                anomalies.append(True)\n",
    "         else:\n",
    "            anomalies.append(isAnomaly)             \n",
    "    #return  mae, deviation,addHeader(ts,adata)\n",
    "    return  ts,adata, anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_test, adata_test, anomalies = detectLowerUpperAnomalies(test_predict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plat_test = [date2num(d)  for d in ts_test]\n",
    "plat_train = [date2num(d)  for d in train.index.get_values()]\n",
    "plat_test_origin = [date2num(d)  for d in test.index.get_values()]\n",
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "plt.plot_date(plat_train, train.y,color=\"blue\", linewidth=2.5, linestyle=\"-\", label='train')\n",
    "plt.plot_date(plat_test_origin, test.y,color=\"green\", linewidth=2.5, linestyle=\"-\", label='test')\n",
    "\n",
    "plt.plot_date(plat_test, adata_test,color=\"red\", linewidth=2.5, linestyle=\"-\", label='abnomalies')\n",
    "\n",
    "plt.title('Anomaly Data trend (train test splits 0.3)--PROPHET--')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
